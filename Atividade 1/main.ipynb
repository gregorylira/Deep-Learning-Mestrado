{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lista De Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grupo:\n",
    "- Gregory Filipe Lira da Silva\n",
    "- Danilo Henrique da Silva Santana\n",
    "- Dayvison Gomes de Oliveira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from random import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from copy import deepcopy\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "from neural_network import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "def __compute_meshgrid(x, y):\n",
    "    x_min, x_max, y_min, y_max = x[:, 0].min(), x[:, 0].max(), x[:, 1].min(), x[:, 1].max()\n",
    "    x1, x2 = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    x_mesh = np.array([x1.ravel(), x2.ravel()]).T\n",
    "    return x1, x2, x_mesh\n",
    "\n",
    "def classification_predictions(x, y, is_binary, nn=None, threshold=0.0, figsize=(12,6), s=15, cmap=plt.cm.viridis):\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=list(np.array(y).ravel()), s=s, cmap=cmap)\n",
    "\n",
    "    if nn is not None:\n",
    "        plt.subplot(1, 2, 2, sharex=ax, sharey=ax)\n",
    "\n",
    "        x1, x2, x_mesh = __compute_meshgrid(x, y)\n",
    "        y_mesh = nn.predict(x_mesh)\n",
    "        y_mesh = np.where(y_mesh <= threshold, 0, 1) if is_binary else np.argmax(__softmax(y_mesh), axis=1)\n",
    "\n",
    "        plt.scatter(x[:, 0], x[:, 1], c=list(np.array(y).ravel()), s=s, cmap=cmap)\n",
    "        plt.contourf(x1, x2, y_mesh.reshape(x1.shape), cmap=cmap, alpha=0.5)\n",
    "\n",
    "        \n",
    "def make_log10(n_samples, x_min, x_max, noise=0.0, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    x = np.logspace(np.log10(x_min), np.log10(x_max), n_samples)\n",
    "    y = np.log10(x) + 2*noise*np.random.random(n_samples) - noise\n",
    "    return x.reshape(-1,1), y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) A representação de uma determinada mensagem digital ternária, isto é formada por três bits,\n",
    "forma um cubo cujos vértices correspondem a mesma representação digital. Supondo que ao\n",
    "transmitirmos esta mensagem a mesma possa ser contaminada por ruído formado em torno de\n",
    "cada vértice uma nuvem esférica de valores aleatórios com raio máximo é 0.1. Formule este\n",
    "problema como um problema de classificação de padrões e treine uma Rede Perceptron de\n",
    "Rosenblatt (Perceptron de camada única) para atuar como classificador/decodificador. Para\n",
    "solução do problema defina antes um conjunto de treinamento e um conjunto de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,0,0], [0,0,1], [0,1,0], [0,1,1], [1,0,0], [1,0,1], [1,1,0],[1,1,1]])\n",
    "print(x.shape)\n",
    "classes = [ i for i in range(x.shape[0])] # para organizar e colocar cada um das 8 classes\n",
    "data = pd.DataFrame(np.column_stack((x,classes)), columns=['x1','x2','x3','class'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_dados(quantidade, base):\n",
    "    dados_gerados = deepcopy(base)\n",
    "    valores = dados_gerados[['x1','x2','x3']].values\n",
    "    \n",
    "    for i in range(quantidade):\n",
    "        ruido = np.array([np.random.uniform(-0.1,0.1) for i in range(x.shape[0]*x.shape[1])]).reshape(8,3)\n",
    "        novos_valores = valores + ruido\n",
    "        for k in range(len(novos_valores)):\n",
    "            #adicionando novos dados no final do dataset com o ruido\n",
    "            dados_gerados.loc[len(dados_gerados)] = [novos_valores[k][0],novos_valores[k][1],novos_valores[k][2],k]\n",
    "            \n",
    "    return dados_gerados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_novos = gerar_dados(500,data)\n",
    "dados_novos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = dados_novos['class'].unique()\n",
    "\n",
    "figure = plt.figure(figsize=(20,10))\n",
    "axis = figure.add_subplot(111, projection='3d')\n",
    "\n",
    "for classe in classes:\n",
    "    valores =  dados_novos[dados_novos['class'] == classe].values\n",
    "    x = valores[:,0]\n",
    "    y = valores[:,1]\n",
    "    z = valores[:,2]\n",
    "    axis.scatter(x,y,z, label=classe)\n",
    "    \n",
    "axis.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dados_novos[['x1','x2','x3']].values\n",
    "y = dados_novos[['class']].values.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.3)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceptron()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(8 , X.shape[1]) # [8x3] 8 caracteristica para 3 colunas de entrada\n",
    "learning_rate = 1e-3\n",
    "\n",
    "for step in range(201):\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(X_train,y_train):\n",
    "        y_pred = np.dot(x_n,w.T)\n",
    "        y_pred = np.argmax(y_pred)\n",
    "        error = y_n - y_pred\n",
    "        #atualizar os pesos, se ele errou, quando y_n é diferente de y_pred (por isso atualizar os pesos nas posições respectivas)\n",
    "        if not (y_n == y_pred):\n",
    "            w[int(y_n)] +=  learning_rate* x_n # adiciona importancia a entrada\n",
    "            w[int(y_pred)] -=  learning_rate* x_n #  diminui a importancia pois errou \n",
    "\n",
    "        cost += error**2\n",
    "    if step % 10 == 0:\n",
    "        print('epoch: {0}/{1} loss_train: {2:.4f}'.format(step,200,0.5*(cost/(len(X_train)))))\n",
    "\n",
    "print('w: ',w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax( np.dot(X_test, w.T) , axis=1 )\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Implemente uma rede perceptron de múltiplas camadas e utilize-a para aproximar as\n",
    "funções abaixo. Em seguida, compare os resultados com as curvas exatas. No caso das letras\n",
    "(b) e (c), apresente também a curva do erro médio de treinamento com relação ao número de\n",
    "épocas e a curva do erro médio com o conjunto de validação.\n",
    "\n",
    "    a) a função lógica XOR\n",
    "\n",
    "    b) f(x) = log10(x), onde 1 ≤ x ≤ 10\n",
    "\n",
    "    c) f(x) = 10x^5+ 5x^4+ 2x^3– 0.5x^2+ 3x + 2, onde 0 ≤ x ≤ 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) Função lógica XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_dados_xor(quantidade, base):\n",
    "    dados_gerados = deepcopy(base)\n",
    "    valores = dados_gerados[['x0','x1']].values\n",
    "    \n",
    "    for i in range(quantidade):\n",
    "        ruido = np.array([np.random.uniform(-0.1,0.1) for i in range(x.shape[0]*x.shape[1])]).reshape(4,2)\n",
    "        novos_valores = valores + ruido\n",
    "        for k in range(len(novos_valores)):\n",
    "            #adicionando novos dados no final do dataset com o ruido\n",
    "            if (k == 2):\n",
    "                dados_gerados.loc[len(dados_gerados)] = [novos_valores[k][0],novos_valores[k][1],1]\n",
    "            elif (k == 3):\n",
    "                dados_gerados.loc[len(dados_gerados)] = [novos_valores[k][0],novos_valores[k][1],0]\n",
    "            else:\n",
    "                dados_gerados.loc[len(dados_gerados)] = [novos_valores[k][0],novos_valores[k][1],k]\n",
    "            \n",
    "            \n",
    "    return dados_gerados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0]).reshape(-1, 1)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "plt.scatter(x[:,0], x[:,1], c=list(np.array(y).ravel()), s=15, cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.DataFrame(np.column_stack((x,y)), columns=[\"x0\",\"x1\",\"y\"])\n",
    "novos_dados = gerar_dados_xor(500, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = novos_dados['y'].unique()\n",
    "\n",
    "figure = plt.figure(figsize=(20,10))\n",
    "axis = figure.add_subplot(111)\n",
    "\n",
    "for classe in classes:\n",
    "    valores =  novos_dados[novos_dados['y'] == classe].values\n",
    "    x = valores[:,0]\n",
    "    y = valores[:,1]\n",
    "    axis.scatter(x,y, label=classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = novos_dados[[\"x0\",\"x1\"]].values\n",
    "y = novos_dados[[\"y\"]].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size= 0.3)\n",
    "\n",
    "x.shape, y.shape # mostrando a quantidade de dados utilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(activation='relu', max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# print('Predições:', y_pred, sep='\\n')\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y_test, y_pred > 0.5)))\n",
    "\n",
    "classification_predictions(X_test, y_pred, is_binary=True, threshold=0.5, nn=model, cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(model.loss_curve_)\n",
    "plt.title('Metrica de erro')\n",
    "plt.ylabel('Erro')\n",
    "plt.xlabel('Epoca')\n",
    "plt.legend(['Treinamento'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rede Neural implementada\n",
    "\n",
    "input_dim, output_dim = X_train.shape[1], y_train.shape[1]\n",
    "\n",
    "nn = NeuralNetwork(cost_func = binary_cross_entropy, learning_rate= 1e-1)\n",
    "nn.layers.append(Layer(input_dim=input_dim, output_dim=10, activation=sigmoid))\n",
    "nn.layers.append(Layer(input_dim=10, output_dim=output_dim, activation=sigmoid))\n",
    "\n",
    "nn.fit(X_train,y_train, X_test, y_test,True, epochs=3000, verbose=300)\n",
    "\n",
    "y_pred = nn.predict(x)\n",
    "print('Predições:', y_pred, sep='\\n')\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5)))\n",
    "\n",
    "classification_predictions(x, y, is_binary=True, threshold=0.5, nn=nn, cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.train_val_loss['loss'])\n",
    "plt.plot(nn.train_val_loss['val_loss'])\n",
    "plt.title('Metrica de erro')\n",
    "plt.ylabel('Erro')\n",
    "plt.xlabel('Epoca')\n",
    "plt.legend(['Treinamento', 'Validacao'])\n",
    "plt.show()\n",
    "\n",
    "pred = np.round(nn.predict(X_test))\n",
    "pred_train = np.round(nn.predict(X_train))\n",
    "\n",
    "print(f'--> Acuracia (train): {accuracy_score(y_train, pred_train):.4f}')\n",
    "print(f'--> Acuracia (test): {accuracy_score(y_test, pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) f(x) = log10(x), onde 1 ≤ x ≤ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = make_log10(100, 1, 10, noise=0, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizar os dados\n",
    "minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "x = minmax.fit_transform(x.astype(np.float64))\n",
    "\n",
    "x.shape, y.shape # mostrando a quantidade de dados utilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "regr = MLPRegressor( max_iter=3000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(regr.loss_curve_)\n",
    "plt.title('Metrica de erro')\n",
    "plt.ylabel('Erro')\n",
    "plt.xlabel('Epoca')\n",
    "plt.legend(['Treinamento'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(1)))\n",
    "model.add(tf.keras.layers.Dense(10, activation='tanh'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='tanh'))\n",
    "model.add(tf.keras.layers.Dense(1,activation='linear'))\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(loss='mean_squared_error', metrics='mean_squared_error', optimizer='adam')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Metrica de erro')\n",
    "plt.ylabel('Erro')\n",
    "plt.xlabel('Epoca')\n",
    "plt.legend(['Treinamento', 'Validacao'])\n",
    "plt.show()\n",
    "\n",
    "pred = np.round(model.predict(X_test))\n",
    "pred_train = np.round(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilizando a Rede Neural Implementada\n",
    "input_dim, output_dim = X_train.shape[1], y_train.shape[1]\n",
    "\n",
    "\n",
    "nn = NeuralNetwork(cost_func = mse, learning_rate=1e-2)\n",
    "nn.layers.append(Layer(input_dim=input_dim, output_dim= 10, activation = tanh))\n",
    "nn.layers.append(Layer(input_dim=10, output_dim= output_dim, activation = linear))\n",
    "\n",
    "nn.fit(X_train,y_train, X_test, y_test, True, epochs=3000, verbose= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.train_val_loss['loss'])\n",
    "plt.plot(nn.train_val_loss['val_loss'])\n",
    "plt.title('Metrica de erro')\n",
    "plt.ylabel('Erro')\n",
    "plt.xlabel('Epoca')\n",
    "plt.legend(['Treinamento', 'Validacao'])\n",
    "plt.show()\n",
    "\n",
    "pred = np.round(nn.predict(X_test))\n",
    "pred_train = np.round(nn.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, output_dim = x.shape[1], y.shape[1]\n",
    "\n",
    "\n",
    "nn = NeuralNetwork(cost_func = mse, learning_rate=1e-2)\n",
    "nn.layers.append(Layer(input_dim=input_dim, output_dim= 10, activation = tanh))\n",
    "nn.layers.append(Layer(input_dim=10, output_dim= output_dim, activation = linear))\n",
    "\n",
    "nn.fit(x,y, epochs=3000, verbose= 300)\n",
    "\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x, nn.predict(x), c=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) f(x) = 10x^5+ 5x^4+ 2x^3– 0.5x^2+ 3x + 2, onde 0 ≤ x ≤ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,100)\n",
    "y = 10*x**5 + 5*x**4 + 2*x**3 - 0.5*x**2 + 3*x + 2 \n",
    "\n",
    "minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "x = minmax.fit_transform(x.reshape(-1,1).astype(np.float64))\n",
    "\n",
    "# x = x.reshape(-1,1)\n",
    "y = y.reshape(-1,1)\n",
    "x.shape, y.shape # mostrando a quantidade de dados utilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "regr = MLPRegressor( max_iter=3000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(regr.loss_curve_)\n",
    "plt.title('Metrica de erro')\n",
    "plt.ylabel('Erro')\n",
    "plt.xlabel('Epoca')\n",
    "plt.legend(['Treinamento'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(1)))\n",
    "model.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1,activation='linear'))\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(loss='mean_squared_error', metrics='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Metrica de erro')\n",
    "plt.ylabel('Erro')\n",
    "plt.xlabel('Epoca')\n",
    "plt.legend(['Treinamento', 'Validacao'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, output_dim = X_train.shape[1], y_train.shape[1]\n",
    "\n",
    "nn = NeuralNetwork(cost_func = mse, learning_rate = 1e-3)\n",
    "nn.layers.append(Layer(input_dim=input_dim, output_dim=12, activation=relu))\n",
    "nn.layers.append(Layer(input_dim=12, output_dim=12, activation=relu))\n",
    "nn.layers.append(Layer(input_dim=12, output_dim=output_dim, activation=linear))\n",
    "\n",
    "nn.fit(X_train,y_train, X_test, y_test, True, epochs=3000, verbose=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Metrica de erro')\n",
    "plt.ylabel('Erro')\n",
    "plt.xlabel('Epoca')\n",
    "plt.legend(['Treinamento', 'Validacao'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, output_dim = x.shape[1], y.shape[1]\n",
    "\n",
    "nn = NeuralNetwork(cost_func = mse, learning_rate = 1e-3)\n",
    "nn.layers.append(Layer(input_dim=input_dim, output_dim=12, activation=relu))\n",
    "nn.layers.append(Layer(input_dim=12, output_dim=12, activation=relu))\n",
    "nn.layers.append(Layer(input_dim=12, output_dim=output_dim, activation=linear))\n",
    "\n",
    "nn.fit(x,y, epochs=3000, verbose=300)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x, nn.predict(x), c=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considere um problema de classificação de padrões constituído de oito padrões. A\n",
    "distribuição dos padrões forma um círculo centrado na origem de raio unitário e contido no\n",
    "círculo um losango também centrado na origem e com lados iguais à raiz de 2. Os dados das\n",
    "classes C1, C2, C3, C4 correspondem aos quatro setores do losango e as outras quatro\n",
    "classes correspondem aos setores delimitados pelo círculo e os lados do losango. Após gerar\n",
    "aleatoriamente dados que venham formar estas distribuições de dados, selecione um conjunto\n",
    "de treinamento e um conjunto de validação. Treine duas redes perceptron (uma rede\n",
    "utilizando a regra delta convencional, e outra usando a regra delta com termo do\n",
    "momento), para classificar os padrões associados a cada uma das classes. Verifique o\n",
    "desempenho dos classificadores usando o conjunto de validação e calculando a matriz de\n",
    "confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_data_problem(n_exemplos):\n",
    "    x = np.random.uniform(-1,1,n_exemplos)\n",
    "    y = np.random.uniform(-1,1,n_exemplos)\n",
    "    \n",
    "    dados = pd.DataFrame(data={'x':[0.1], 'y': [0.1], 'Class': [0]})\n",
    "    \n",
    "    \n",
    "    for i in range(n_exemplos):\n",
    "        if ( y[i] <= 1 - x[i] and x[i] >= 0 and y[i] >= 0 ):\n",
    "            dados.loc[len(dados)] = [x[i],y[i],0]\n",
    "            \n",
    "        elif ( y[i] <= x[i] + 1 and x[i] <= 0 and y[i]>=0 ):\n",
    "            dados.loc[len(dados)] = [x[i],y[i],1]\n",
    "            \n",
    "        elif ( y[i] >= -x[i] - 1 and x[i] <= 0 and y[i] <= 0 ):\n",
    "            dados.loc[len(dados)] = [x[i],y[i],2]\n",
    "            \n",
    "        elif ( y[i] >= x[i] - 1 and x[i] >= 0 and y[i] <= 0 ):\n",
    "            dados.loc[len(dados)] = [x[i],y[i],3]\n",
    "            \n",
    "        elif ( y[i] >= 1 - x[i] and x[i] >= 0 and y[i] >= 0  and x[i]**2 + y[i]**2 <=1 ):\n",
    "            dados.loc[len(dados)] = [x[i],y[i],4]\n",
    "            \n",
    "        elif ( y[i] >= x[i] + 1 and x[i] <= 0 and y[i]>=0 and x[i]**2 + y[i]**2 <=1 ):\n",
    "            dados.loc[len(dados)] = [x[i],y[i],5]\n",
    "            \n",
    "        elif ( y[i] <= -x[i] - 1 and x[i] <= 0 and y[i] <= 0 and x[i]**2 + y[i]**2 <=1 ):\n",
    "            dados.loc[len(dados)] = [x[i],y[i],6]\n",
    "            \n",
    "        elif ( y[i] <= x[i] - 1 and x[i] >= 0 and y[i] <= 0 and x[i]**2 + y[i]**2 <=1 ):\n",
    "            dados.loc[len(dados)] = [x[i],y[i],7]\n",
    "            \n",
    "            \n",
    "    return dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_gerados = Generate_data_problem(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = dados_gerados['Class'].unique()\n",
    "    \n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for classe in classes:\n",
    "    valores = dados_gerados[dados_gerados['Class'] == classe].values\n",
    "    x,y = valores[:,0] , valores[:,1]\n",
    "    ax.scatter(x,y, label=classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dados_gerados[['x','y']].values,dados_gerados[['Class']].values, test_size=0.3)\n",
    "\n",
    "\n",
    "dados_gerados[['x','y']].shape, dados_gerados[['Class']].shape # mostrando a quantidade de dados utilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = OneHotEncoder(sparse=False)\n",
    "\n",
    "y_train = one.fit_transform(y_train.reshape(-1,1))\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "regr = MLPRegressor( max_iter=3000).fit(X_train, y_train)\n",
    "\n",
    "y_pred = regr.predict(X_test)\n",
    "one.inverse_transform(y_pred)\n",
    "pred = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test,pred))\n",
    "print('Matriz de Confusão: \\n')\n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(regr.loss_curve_)\n",
    "plt.title('Metrica de erro')\n",
    "plt.ylabel('Erro')\n",
    "plt.xlabel('Epoca')\n",
    "plt.legend(['Treinamento'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "regr = MLPRegressor( max_iter=3000, momentum=0.52).fit(X_train, y_train)\n",
    "\n",
    "y_pred = regr.predict(X_test)\n",
    "one.inverse_transform(y_pred)\n",
    "pred = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test,pred))\n",
    "print('Matriz de Confusão: \\n')\n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(regr.loss_curve_)\n",
    "plt.title('Metrica de erro')\n",
    "plt.ylabel('Erro')\n",
    "plt.xlabel('Epoca')\n",
    "plt.legend(['Treinamento'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, output_dim = X_train.shape[1], y_train.shape[1]\n",
    "\n",
    "nn = NeuralNetwork(cost_func = softmax_neg_log_likelihood, learning_rate = 1e-1)\n",
    "\n",
    "nn.layers.append(Layer(input_dim=input_dim, output_dim=10, activation= tanh))\n",
    "nn.layers.append(Layer(input_dim=10, output_dim=10, activation= relu))\n",
    "nn.layers.append(Layer(input_dim=10, output_dim=output_dim, activation= linear))\n",
    "\n",
    "nn.fit(X_train,y_train, epochs=3000, verbose=300)\n",
    "\n",
    "y_pred = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "print('Matriz de Confusão: \\n')\n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict(X_test[0])\n",
    "np.argmax(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
